{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtcRNhlVIlnGmKGoko5nKa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdJbIwkSJB55","executionInfo":{"status":"ok","timestamp":1726817464911,"user_tz":-240,"elapsed":318,"user":{"displayName":"Arevik Khachatryan","userId":"06400131763786536191"}},"outputId":"ee12050d-8367-49eb-d694-6980f8f7481b"},"outputs":[{"output_type":"stream","name":"stdout","text":["The list of tokens after step 1 (Tokenization):\n"," \n","['Natural', 'Language', 'Processing', '(NLP)', 'is', 'a', 'subfield', 'of', 'linguistics,', 'computer', 'science,', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language.', \"It's\", 'used', 'to', 'analyze', 'text,', 'allowing', 'machines', 'to', 'understand,', 'interpret,', 'and', 'manipulate', 'human', 'language.', 'NLP', 'has', 'many', 'real-world', 'applications,', 'including', 'machine', 'translation,', 'sentiment', 'analysis,', 'and', 'chatbots.']\n","The list of tokens after step 2 (Lowercasing):\n"," \n","['natural', 'language', 'processing', '(nlp)', 'is', 'a', 'subfield', 'of', 'linguistics,', 'computer', 'science,', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language.', \"it's\", 'used', 'to', 'analyze', 'text,', 'allowing', 'machines', 'to', 'understand,', 'interpret,', 'and', 'manipulate', 'human', 'language.', 'nlp', 'has', 'many', 'real-world', 'applications,', 'including', 'machine', 'translation,', 'sentiment', 'analysis,', 'and', 'chatbots.']\n","Changed words:\n"," \n","nlp)\n","nlp\n","linguistics\n","science\n","language\n","its\n","text\n","understand\n","interpret\n","language\n","realworld\n","applications\n","translation\n","analysis\n","chatbots\n","\n","The list of tokens after step 3 (Pumctuation Removal):\n"," \n","['natural', 'language', 'processing', 'nlp', 'is', 'a', 'subfield', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', 'its', 'used', 'to', 'analyze', 'text', 'allowing', 'machines', 'to', 'understand', 'interpret', 'and', 'manipulate', 'human', 'language', 'nlp', 'has', 'many', 'realworld', 'applications', 'including', 'machine', 'translation', 'sentiment', 'analysis', 'and', 'chatbots']\n","\n","The list of tokens after step 4 (Stop Word Removal):\n"," \n","['natural', 'language', 'processing', 'nlp', 'subfield', 'of', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'between', 'computers', 'human', 'language', 'its', 'used', 'to', 'analyze', 'text', 'allowing', 'machines', 'to', 'understand', 'interpret', 'manipulate', 'human', 'language', 'nlp', 'has', 'many', 'realworld', 'applications', 'including', 'machine', 'translation', 'sentiment', 'analysis', 'chatbots']\n","\n","The list of tokens after step 5 (Stemming):\n"," \n","['natural', 'language', 'proces', 'nlp', 'subfield', 'of', 'linguistic', 'computer', 'science', 'artificial', 'intelligence', 'concern', 'interaction', 'between', 'computer', 'human', 'language', 'it', 'u', 'to', 'analyze', 'text', 'allow', 'machin', 'to', 'understand', 'interpret', 'manipulate', 'human', 'language', 'nlp', 'ha', 'many', 'realworld', 'application', 'includ', 'machine', 'translation', 'senti', 'analysi', 'chatbot']\n"]}],"source":["text = \"It's a simple text, for tokenization, are it good enough?\"\n","text_from_Karen = \"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. It's used to analyze text, allowing machines to understand, interpret, and manipulate human language. NLP has many real-world applications, including machine translation, sentiment analysis, and chatbots.\"\n","\n","#Step 1: Tokenization\n","# using split function to split text to words using spases\n","vector = text_from_Karen.split()\n","print(\"The list of tokens after step 1 (Tokenization):\\n \")\n","print(vector)\n","\n","#Step 2: Lowercasing\n","#using lower function for every word\n","lowercased = [x.lower() for x in vector]\n","print(\"The list of tokens after step 2 (Lowercasing):\\n \")\n","print(lowercased)\n","\n","#Step 3: Pumctuation Removal\n","#creating empty array, find ascii for every simvol in every word, chech if\n","#charecter not in lowercase, remove it from word, add words to new array\n","without_punc =[]\n","\n","print(\"Changed words:\\n \")\n","\n","for word in lowercased:\n","  for char in word:\n","    to_ascii = ord(char)\n","    if to_ascii < 97 or to_ascii > 122:\n","      word = word.replace(char, \"\")\n","      print(word)\n","  without_punc.append(word)\n","print(\"\\nThe list of tokens after step 3 (Pumctuation Removal):\\n \")\n","print(without_punc)\n","\n","#Step 4: Stop Word Removal\n","#defining stop words, create empty array for text without stop words, for every\n","#word chech if it not stop word, add it to new array\n","stop_words = [\"the\", \"a\", \"and\", \"an\", \"is\", \"but\", \"or\", \"in\", \"on\", \"at\",\n","              \"with\", \"he\", \"she\", \"it\", \"they\", \"am\", \"are\", \"was\", \"where\",\n","              \"be\", \"being\", \"been\"]\n","\n","filtered = []\n","\n","for word in without_punc:\n","  if word not in stop_words:\n","    filtered.append(word)\n","print(\"\\nThe list of tokens after step 4 (Stop Word Removal):\\n \")\n","print(filtered)\n","\n","#Step 5: Stemming\n","#defining stemming suffixes, create empty array for stemmed words, for every\n","#word and for every suffix check if word has that suffix, remove it from word,\n","#add words to new array\n","stemming_suffixes = [\"ing\", \"ly\", \"ed\", \"ious\", \"ies\", \"ive\", \"es\", \"s\", \"ment\"]\n","\n","stemmed = []\n","\n","for word in filtered:\n","  for suffix in stemming_suffixes:\n","    if word.endswith(suffix):\n","      word = word[:-len(suffix)]\n","  stemmed.append(word)\n","print(\"\\nThe list of tokens after step 5 (Stemming):\\n \")\n","print(stemmed)\n","\n","#Bonus (optional)\n","#it's cant work, because every word in dict is stop_word\n","\n","# lemmatization_dict = {\"are\": \"be\", \"am\": \"be\", \"is\": \"be\", \"was\": \"be\", \"were\": \"be\"}\n","\n","# lemmatized = []\n","\n","# for word in stemmed:\n","#   if word in lemmatization_dict:\n","#     word = lemmatization_dict[word]\n","#   lemmatized.append(word)\n","# print(lemmatized)"]}]}
